{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd063dd-8488-4491-8ca3-ae2c1fe179e2",
   "metadata": {},
   "source": [
    "# Companies House Public Data API Project Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4aebcc9-19b3-4b70-93dc-895d5a84d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making HTTP requests to the Companies House API\n",
    "import requests\n",
    "# for working with JASON data (parsing API responses, encoding addresses)\n",
    "import json\n",
    "# for adding delays between API called to avoid rate limiting\n",
    "import time\n",
    "# for writing data to a csv file in a structured format\n",
    "import csv\n",
    "# for interacting with the operating system (managing file paths or environment variables)\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95deffe6-626e-4770-a6fa-2078d8a203aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(filepath=\"config.txt\"):\n",
    "    \"\"\"\n",
    "    Load an API Key from a config file.\n",
    "    The file must contain a line likeL API_KEY=your-key-here\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"API_KEY=\"):\n",
    "                return line.strip().split(\"=\")[1]\n",
    "    raise ValueError(\"API_KEY not found in config file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96140eaf-e813-4930-b0cd-c1820d1e68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API key from config file - this avoids hardcoding the key into the script\n",
    "API_KEY = load_api_key()\n",
    "# set the base URL for all API calls\n",
    "BASE_URL = \"https://api.company-information.service.gov.uk\"\n",
    "# specify that the client expects JSON responses from the API\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "# file name where all fetched company profile data will be saved as a CSV\n",
    "OUTPUT_FILE = \"company_profiles.csv\"\n",
    "# time delay (in seconds) between consecutive API requests to avoid hitting rate limits\n",
    "DELAY_BETWEEN_REQUESTS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf6fa39-da42-42df-bfe8-eb3265aff1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Accepted cookie consent.\n",
      "[INFO] Scraping page 1...\n",
      "[INFO] Found 20 rows.\n",
      "[INFO] Scraping page 2...\n",
      "[INFO] Found 20 rows.\n",
      "[INFO] Scraping page 3...\n",
      "[INFO] Found 20 rows.\n",
      "[INFO] Scraping page 4...\n",
      "[INFO] Found 20 rows.\n",
      "[INFO] Scraping page 5...\n",
      "[INFO] Found 20 rows.\n",
      "\n",
      "Retrieved 100 companies from LSE FTSE 100.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "\n",
    "# URL of the FTSE 100 constituent table\n",
    "url = \"https://www.londonstockexchange.com/indices/ftse-100/constituents/table\"\n",
    "\n",
    "# set up Chrome options\n",
    "options = Options()\n",
    "# ensure full content loads properly\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# launch the Chrome browser with the specified options\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "\n",
    "# navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# set up an explicit wait of 15 seconds to be used throughout the script\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "# --- Accept Cookie Consent ---\n",
    "\n",
    "try:\n",
    "    # wait for the \"Accept all cookies\" button to be clickable and then click it\n",
    "    cookie_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='Accept all cookies']\")))\n",
    "    cookie_btn.click()\n",
    "    print(\"[INFO] Accepted cookie consent.\")\n",
    "except Exception as e:\n",
    "    # if the button doesn't appear or isn't clickable, continue without crashing\n",
    "    print(f\"[WARN] Cookie button not found: {e}\")\n",
    "\n",
    "# --- Scrape All 5 Pages ---\n",
    "\n",
    "# initialise a list to store tuples\n",
    "data = []\n",
    "\n",
    "# loop through pages 1 to 5\n",
    "for page_num in range(1, 6):\n",
    "    print(f\"[INFO] Scraping page {page_num}...\")\n",
    "\n",
    "    try:\n",
    "        # wait until the table rows (one per company) are loaded\n",
    "        wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \"tr.medium-font-weight\")\n",
    "        ))\n",
    "        time.sleep(1)  # slight pause to ensure rows are populated\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \"tr.medium-font-weight\")\n",
    "        print(f\"[INFO] Found {len(rows)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Table not found on page {page_num}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # extract ticker and company name from each row\n",
    "    for row in rows:\n",
    "        try:\n",
    "            ticker = row.find_element(By.CSS_SELECTOR, \"td.instrument-tidm a\").text.strip()\n",
    "            name = row.find_element(By.CSS_SELECTOR, \"td.instrument-name a\").text.strip()\n",
    "            currency = row.find_element(By.CSS_SELECTOR, \"td.instrument-currency\").text.strip()\n",
    "            market_cap = row.find_element(By.CSS_SELECTOR, \"td.instrument-marketcapitalization\").text.strip()\n",
    "            price = row.find_element(By.CSS_SELECTOR, \"td.instrument-lastprice\").text.strip()\n",
    "            change = row.find_element(By.CSS_SELECTOR, \"td.instrument-netchange\").text.strip()\n",
    "            change_pct = row.find_element(By.CSS_SELECTOR, \"td.instrument-percentualchange\").text.strip()\n",
    "\n",
    "            data.append((ticker, name, currency, market_cap, price, change, change_pct))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to parse a row: {e}\")\n",
    "\n",
    "    # click the next page link unless we're on the last page\n",
    "    if page_num < 5:\n",
    "        try:\n",
    "            # wait for the link to the next page number to be clickable\n",
    "            next_button = wait.until(EC.element_to_be_clickable(\n",
    "                (By.XPATH, f\"//a[@class='page-number' and text()='{page_num + 1}']\")\n",
    "            ))\n",
    "            # scroll to it as some buttons won't click if off-screen\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(0.5) # small delay before clicking\n",
    "            next_button.click()\n",
    "            time.sleep(2.5) # allow the new page to load\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not navigate to page {page_num + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "# close the browser after scraping\n",
    "driver.quit()\n",
    "\n",
    "# --- Save to CSV ---\n",
    "\n",
    "# write the scraped data to a CSV file\n",
    "with open(\"ftse100_lse.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Ticker\", \"Company Name\", \"Currency\", \"Market Cap (m)\", \"Price\", \"Change\", \"Change %\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"\\nRetrieved {len(data)} companies from LSE FTSE 100.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7678fb-8985-4711-bd35-b17efda9e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     ftse_companies = get_ftse100_from_lse(save_to_csv=True)\n",
    "#     print(f\"Retrieved {len(ftse_companies)} FTSE 100 companies.\")\n",
    "#     for name, ticker in ftse_companies[:40]:\n",
    "#         print(f\"- {name} ({ticker})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c63b952-5cb0-4771-ac69-d3fbc15fc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetches the core company profile from the Companies House API\n",
    "def fetch_company_profile(company_number):\n",
    "    # construct the API URL for the company's main profile\n",
    "    url = f\"{BASE_URL}/company/{company_number}\"\n",
    "    try:\n",
    "        # send GET request with basic auth (API key only)\n",
    "        response = requests.get(url, auth=(API_KEY, \"\"), headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            # return JSON data if successful\n",
    "            return response.json()\n",
    "        else:\n",
    "            # log status code if note successful\n",
    "            print(f\"[{response.status_code}] Failed to fetch {company_number}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # catch and log connection or parsing errors\n",
    "        print(f\"[ERROR] {company_number}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# retrieves a list of up to 3 officer (director) names for the company\n",
    "def fetch_officers(company_number):\n",
    "    url = f\"{BASE_URL}/company/{company_number}/officers\"\n",
    "    response = requests.get(url, auth=(API_KEY, \"\"), headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # limit to first 3 officers\n",
    "        officers = data.get(\"items\", [])[:3]\n",
    "        # return only officer names, filtering out any nulls\n",
    "        return [str(o.get(\"name\", \"Unknown\")) for o in officers if o.get(\"name\")]\n",
    "    # return empty list if request fails or no officers\n",
    "    return []\n",
    "\n",
    "# retrieves up to 5 most recent filing history entries\n",
    "def fetch_filing_history(company_number):\n",
    "    url = f\"{BASE_URL}/company/{company_number}/filing-history\"\n",
    "    response = requests.get(url, auth=(API_KEY, \"\"), headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # limit to 5 filings\n",
    "        filings = data.get(\"items\", [])[:5]\n",
    "        # format: \"filing_type on date\"\n",
    "        return [f\"{str(f.get('type', 'UNKNOWN'))} on {str(f.get('date', 'UNKNOWN'))}\" for f in filings]\n",
    "    return []\n",
    "\n",
    "# retrieves up to 3 charge entries (e.g. mortgage or secured lending)\n",
    "def fetch_charges(company_number):\n",
    "    url = f\"{BASE_URL}/company/{company_number}/charges\"\n",
    "    response = requests.get(url, auth=(API_KEY, \"\"), headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # limits to 3 charges\n",
    "        charges = data.get(\"items\", [])[:3]\n",
    "        # get charge codes, safely convert to string\n",
    "        return [str(c.get(\"charge_code\", \"Unknown\")) for c in charges if c.get(\"charge_code\")]\n",
    "    return []\n",
    "\n",
    "# retrieves up to 3 names of Persons with Significant Control (PSC)\n",
    "def fetch_psc(company_number):\n",
    "    url = f\"{BASE_URL}/company/{company_number}/persons-with-significant-control\"\n",
    "    response = requests.get(url, auth=(API_KEY, \"\"), headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # limit to 3 PSC entries\n",
    "        pscs = data.get(\"items\", [])[:3]\n",
    "        # return PSC names if present\n",
    "        return [str(p.get(\"name\", \"Unknown\")) for p in pscs if p.get(\"name\")]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5abd9b-6688-4bec-8ba0-6e72c7df2b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 20 companies...\n",
      "Saved 1 company profiles to company_profiles.csv\n",
      "Saved 2 company profiles to company_profiles.csv\n",
      "Saved 3 company profiles to company_profiles.csv\n",
      "Saved 4 company profiles to company_profiles.csv\n",
      "Saved 5 company profiles to company_profiles.csv\n",
      "Saved 6 company profiles to company_profiles.csv\n",
      "Saved 7 company profiles to company_profiles.csv\n",
      "Saved 8 company profiles to company_profiles.csv\n",
      "Saved 9 company profiles to company_profiles.csv\n",
      "Saved 10 company profiles to company_profiles.csv\n",
      "Saved 11 company profiles to company_profiles.csv\n",
      "Saved 12 company profiles to company_profiles.csv\n",
      "Saved 13 company profiles to company_profiles.csv\n",
      "Saved 14 company profiles to company_profiles.csv\n",
      "Saved 15 company profiles to company_profiles.csv\n",
      "Saved 16 company profiles to company_profiles.csv\n",
      "Saved 17 company profiles to company_profiles.csv\n",
      "Saved 18 company profiles to company_profiles.csv\n",
      "Saved 19 company profiles to company_profiles.csv\n",
      "Saved 20 company profiles to company_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # print the number of companies to be processed\n",
    "    print(f\"Fetching data for {len(COMPANY_NUMBERS)} companies...\")\n",
    "\n",
    "    # initialise an empty list to collect all results (one dict per company)\n",
    "    results = []\n",
    "\n",
    "    # loop over each company number in the list\n",
    "    for number in COMPANY_NUMBERS:\n",
    "        # fetch the company's profile (core metadata)\n",
    "        data = fetch_company_profile(number)\n",
    "\n",
    "        # if data was successfully retrieved, enrich and store it\n",
    "        if data:\n",
    "            results.append({\n",
    "                # extract basic company information from the profile\n",
    "                \"company_number\": data.get(\"company_number\"),\n",
    "                \"company_name\": data.get(\"company_name\"),\n",
    "                \"status\": data.get(\"company_status\"),\n",
    "                \"date_of_creation\": data.get(\"date_of_creation\"),\n",
    "                \"type\": data.get(\"type\"),\n",
    "                \"company_category\": data.get(\"company_category\"),\n",
    "                \"jurisdiction\": data.get(\"jurisdiction\"),\n",
    "\n",
    "                # financial reporting data\n",
    "                \"last_accounts_date\": data.get(\"accounts\", {}).get(\"last_accounts\", {}).get(\"made_up_to\"),\n",
    "                \"next_accounts_due\": data.get(\"accounts\", {}).get(\"next_due\"),\n",
    "\n",
    "                # latest confirmation statement date\n",
    "                \"confirmation_statement_date\": data.get(\"confirmation_statement\", {}).get(\"last_made_up_to\"),\n",
    "\n",
    "                # whether the company has a history of insolvency\n",
    "                \"has_insolvency_history\": data.get(\"has_insolvency_history\"),\n",
    "\n",
    "                # flatten the address JSON structure into a string\n",
    "                \"registered_office_address\": json.dumps(data.get(\"registered_office_address\", {})),\n",
    "\n",
    "                # list of SIC (Standard Industrial Classification) codes, joined into one string\n",
    "                \"sic_codes\": \",\".join(data.get(\"sic_codes\", [])),\n",
    "\n",
    "                # enriched data from additional endpoints\n",
    "                \"officers\": \"; \".join(fetch_officers(number)), # top 3 officers\n",
    "                \"recent_filings\": \"; \".join(fetch_filing_history(number)), # latest 5 filings\n",
    "                \"charges\": \"; \".join(fetch_charges(number)), # top 3 charge codes\n",
    "                \"psc\": \"; \".join(fetch_psc(number)) # up to 3 persons with significant control\n",
    "            })\n",
    "\n",
    "        # wait between requests to avoid hitting rate limits\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        # save the current state of the results to CSV after each company\n",
    "        keys = results[0].keys() if results else [] # get column headers from first result\n",
    "        with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "\n",
    "        # log the progress\n",
    "        print(f\"Saved {len(results)} company profiles to {OUTPUT_FILE}\")\n",
    "\n",
    "# entry point of the script - only runs when file is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3b64b-dd8d-4a2a-b648-ef15e1eaf824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
